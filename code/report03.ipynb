{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report 03\n",
    "#### Matthew Beaudouin-Lafon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "% matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from thinkbayes2 import Pmf, Cdf, Suite, Joint\n",
    "import thinkplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPS Problem \n",
    "#### From Think Bayes Chapter 9\n",
    "\n",
    "> GPS included a (currently disabled) feature called Selective Availability (SA) that adds intentional, time varying errors of up to 100 meters (328 ft) to the publicly available navigation signals. This was intended to deny an enemy the use of civilian GPS receivers for precision weapon guidance.\n",
    "> [...]\n",
    "> Before it was turned off on May 2, 2000, typical SA errors were about 50 m (164 ft) horizontally and about 100 m (328 ft) vertically.[10] Because SA affects every GPS receiver in a given area almost equally, a fixed station with an accurately known position can measure the SA error values and transmit them to the local GPS receivers so they may correct their position fixes. This is called Differential GPS or DGPS. DGPS also corrects for several other important sources of GPS errors, particularly ionospheric delay, so it continues to be widely used even though SA has been turned off. The ineffectiveness of SA in the face of widely available DGPS was a common argument for turning off SA, and this was finally done by order of President Clinton in 2000.\n",
    "\n",
    "Suppose it is 1 May 2000, and you are standing in a field that is 200m square.  You are holding a GPS unit that indicates that your location is 51m north and 15m west of a known reference point in the middle of the field.\n",
    "\n",
    "However, you know that each of these coordinates has been perturbed by a \"feature\" that adds random errors with mean 0 and standard deviation 30m.\n",
    "\n",
    "1) After taking one measurement, what should you believe about your position?\n",
    "\n",
    "Note: Since the intentional errors are independent, you could solve this problem independently for X and Y.  But we'll treat it as a two-dimensional problem, partly for practice and partly to see how we could extend the solution to handle dependent errors.\n",
    "\n",
    "You can start with the code in gps.py.\n",
    "\n",
    "2) Suppose that after one second the GPS updates your position and reports coordinates (48, 90).  What should you believe now?\n",
    "\n",
    "3) Suppose you take 8 more measurements and get:\n",
    "\n",
    "    (11.903060613102866, 19.79168669735705)\n",
    "    (77.10743601503178, 39.87062906535289)\n",
    "    (80.16596823095534, -12.797927542984425)\n",
    "    (67.38157493119053, 83.52841028148538)\n",
    "    (89.43965206875271, 20.52141889230797)\n",
    "    (58.794021026248245, 30.23054016065644)\n",
    "    (2.5844401241265302, 51.012041625783766)\n",
    "    (45.58108994142448, 3.5718287379754585)\n",
    "\n",
    "At this point, how certain are you about your location?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GPS(Suite, Joint):\n",
    "    \"\"\"\n",
    "    Bayesian model for the GPS Problem. Inherits from Suite and Joint.\n",
    "    \"\"\"\n",
    "    \n",
    "    def Likelihood(self, data, hypo):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        x, y = hypo\n",
    "        gpsX, gpsY = data\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lincoln Index Problem \n",
    "#### From John D. Cook\n",
    "\"Suppose you have a tester who finds 20 bugs in your program. You want to estimate how many bugs are really in the program. You know there are at least 20 bugs, and if you have supreme confidence in your tester, you may suppose there are around 20 bugs. But maybe your tester isn't very good. Maybe there are hundreds of bugs. How can you have any idea how many bugs there are? There’s no way to know with one tester. But if you have two testers, you can get a good idea, even if you don’t know how skilled the testers are.\n",
    "\n",
    "\"Suppose two testers independently search for bugs. Let k1 be the number of errors the first tester finds and k2 the number of errors the second tester finds. Let c be the number of errors both testers find. The Lincoln Index estimates the total number of errors as k1 k2 / c [I changed his notation to be consistent with mine].\"\n",
    "\n",
    "So if the first tester finds 20 bugs, the second finds 15, and they find 3 in common, we estimate that there are about 100 bugs. What is the Bayesian estimate of the number of errors based on this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39967 0.59894 0.23978\n"
     ]
    }
   ],
   "source": [
    "# Forward problem\n",
    "p1 = 0.4 # Probability tester finds any given bug\n",
    "p2 = 0.6\n",
    "n = 100.0  # Number of bugs\n",
    "\n",
    "K1 = []\n",
    "K2 = []\n",
    "C = []\n",
    "\n",
    "for i in range(1000):\n",
    "    sample1 = np.random.random(size=n) < p1\n",
    "    sample2 = np.random.random(size=n) < p2\n",
    "    k1 = np.sum(sample1) / n\n",
    "    k2 = np.sum(sample2) / n\n",
    "    c = np.sum(sample1 & sample2) / n\n",
    "    K1.append(k1)\n",
    "    K2.append(k2)\n",
    "    C.append(c)\n",
    "\n",
    "print(sum(K1)/len(K1), sum(K2)/len(K2), sum(C)/len(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes at most 3 arguments (4 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-9480fdcd94da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mlincoln\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLincoln\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes at most 3 arguments (4 given)"
     ]
    }
   ],
   "source": [
    "class Lincoln(Suite, Joint):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def Likelihood(self, data, hypo):\n",
    "        \"\"\"\n",
    "        data: k1, k2, c\n",
    "        hypo: n, p1, p2\n",
    "        \"\"\"\n",
    "        k1, k2, c = data\n",
    "        n, p1, p2 = hypo\n",
    "        like1 = thinkbayes.EvalBinomialPmf(k1, p1, n)\n",
    "        like2 = thinkbayes.EvalBinomialPmf(k2, p2, n)\n",
    "        likeC = thinkbayes.EvalBinomialPmf(kc, p1 * p2, n)\n",
    "        return like1 * like2 * likeC\n",
    "    \n",
    "numSamples = 100.0\n",
    "K1 = [i/numSamples in range(int(numSamples))]\n",
    "K2 = [i/numSamples in range(int(numSamples))]\n",
    "C = np.linspace(20, 200)\n",
    "\n",
    "lincoln = Lincoln(K1, K2, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
